# -*- coding: utf-8 -*-
"""Long-Term Temporal Memory Dynamics Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18NFJw4xf3hCmkiimSMEtSn2RU6o6nx8P
"""

import torch
import torchvision
from torch import nn, optim
import cv2
from torchvision import models

from torch.autograd import Variable

import glob
import numpy as np

import warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning) 

from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms

from sklearn.metrics import accuracy_score
from sklearn.metrics import jaccard_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.utils.multiclass import type_of_target
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

import time, os

!pip install split-folders
import splitfolders

from google.colab import drive
drive.mount('/content/drive')

!unzip drive/My\ Drive/Celeb-DF.zip

path2data = "/content"
sub_folder = "Celeb-DF"
sub_folder_jpg = "Celeb-DF-img"

path2aCatgs = os.path.join(path2data, sub_folder)
listOfCategories = os.listdir(path2aCatgs)
listOfCategories, len(listOfCategories)

for cat in listOfCategories:
    print("category:", cat)
    path2acat = os.path.join(path2aCatgs, cat)
    listOfSubs = os.listdir(path2acat)
    print("number of sub-folders:", len(listOfSubs))
    print("-"*50)

import cv2
import numpy as np
def get_frames(filename, n_frames= 1):
    frames = []
    v_cap = cv2.VideoCapture(filename)
    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_list= np.linspace(0, v_len-1, n_frames+1, dtype=np.int16)
    
    for fn in range(v_len):
        success, frame = v_cap.read()
        if success is False:
            continue
        if (fn in frame_list):
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  
            frames.append(frame)
    v_cap.release()
    return frames, v_len


def store_frames(frames, path2store):
    for ii, frame in enumerate(frames):
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  
        path2img = os.path.join(path2store, str(ii)+".jpg")
        cv2.imwrite(path2img, frame)


extension = ".mp4"
n_frames = 5
for root, dirs, files in os.walk(path2aCatgs, topdown=False):
    for name in files:
        if extension not in name:
            continue
        path2vid = os.path.join(root, name)
        frames, vlen = get_frames(path2vid, n_frames= n_frames)
        path2store = path2vid.replace(sub_folder, sub_folder_jpg)
        path2store = path2store.replace(extension, "")
        print(path2store)
        os.makedirs(path2store, exist_ok= True)
        store_frames(frames, path2store)
    print("-"*50)

path2data = "/content"
sub_folder_jpg = "Celeb-DF-img"
path2ajpgs = os.path.join(path2data, sub_folder_jpg)

def get_vids(path2ajpgs):
    listOfCats = os.listdir(path2ajpgs)
    ids = []
    labels = []
    for catg in listOfCats:
        path2catg = os.path.join(path2ajpgs, catg)
        listOfSubCats = os.listdir(path2catg)
        path2subCats= [os.path.join(path2catg,los) for los in listOfSubCats]
        ids.extend(path2subCats)
        labels.extend([catg]*len(listOfSubCats))
    return ids, labels, listOfCats 

all_vids, all_labels, catgs = get_vids(path2ajpgs)
print(catgs)

labels_dict = {}
ind = 0
for uc in catgs:
  labels_dict[uc] = ind
  ind+=1
labels_dict

from torch.utils.data import Dataset, DataLoader, Subset
import glob
from PIL import Image
import torch
import numpy as np
import random
np.random.seed(2020)
random.seed(2020)
torch.manual_seed(2020)

#Class that helps to divide the dataset in train and testing

class VideoDataset(Dataset):
    def __init__(self, ids, labels, transform):      
        self.transform = transform
        self.ids = ids
        self.labels = labels
    def __len__(self):
        return len(self.ids)
    def __getitem__(self, idx):
        path2imgs=glob.glob(self.ids[idx]+"/*.jpg")
        path2imgs = path2imgs[:timesteps]
        label = labels_dict[self.labels[idx]]
        frames = []
        for p2i in path2imgs:
            frame = Image.open(p2i)
            frames.append(frame)
        
        seed = np.random.randint(1e9)        
        frames_tr = []
        for frame in frames:
            random.seed(seed)
            np.random.seed(seed)
            frame = self.transform(frame)
            frames_tr.append(frame)
        if len(frames_tr)>0:
            frames_tr = torch.stack(frames_tr)
        return frames_tr, label

from sklearn.model_selection import StratifiedShuffleSplit

unique_ids = all_vids
unique_labels = all_labels

sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)
train_indx, test_indx = next(sss.split(unique_ids, unique_labels))

train_ids = [unique_ids[ind] for ind in train_indx]
train_labels = [unique_labels[ind] for ind in train_indx]
print("Train ids: " + str(len(train_ids)) + "\nTrain labels: " + str(len(train_labels)) + "\n") 

test_ids = [unique_ids[ind] for ind in test_indx]
test_labels = [unique_labels[ind] for ind in test_indx]
print("Test ids: " + str(len(test_ids)) + "\nTest labels: " + str(len(test_labels)))

import torchvision.transforms as transforms
timesteps = 5

# choose one
model_type = "3dcnn"
model_type = "rnn"    

timesteps = 12
if model_type == "rnn":
    h, w =224, 224
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
else:
    h, w = 112, 112
    mean = [0.43216, 0.394666, 0.37645]
    std = [0.22803, 0.22145, 0.216989]

train_transformer = transforms.Compose([
            transforms.Resize((h,w)),
            transforms.RandomHorizontalFlip(p=0.5),  
            transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),    
            transforms.ToTensor(),
            transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),
            ])    

test_transformer = transforms.Compose([
            transforms.Resize((h,w)),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
            ])

train_ds = VideoDataset(ids= train_ids, labels= train_labels, transform= train_transformer)
print(len(train_ds))

test_ds = VideoDataset(ids= test_ids, labels= test_labels, transform= test_transformer)
print(len(test_ds))

def collate_fn_r3d_18(batch):
    print(batch)
    imgs_batch, label_batch = list(zip(*batch))
    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]
    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]
    imgs_tensor = torch.stack(imgs_batch)
    imgs_tensor = torch.transpose(imgs_tensor, 2, 1)
    labels_tensor = torch.stack(label_batch)
    return imgs_tensor,labels_tensor

def collate_fn_rnn(batch):
    print(batch)
    imgs_batch, label_batch = list(zip(*batch))
    print(label_batch)
    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]
    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]
    imgs_tensor = torch.stack(imgs_batch)
    labels_tensor = torch.stack(label_batch)
    return imgs_tensor,labels_tensor
    

batch_size = 1
if model_type == "rnn":
    train_dl = DataLoader(train_ds, batch_size= batch_size,
                          shuffle=True, collate_fn= None)
    test_dl = DataLoader(test_ds, batch_size= 2*batch_size,
                         shuffle=False, collate_fn= None)  
else:
    train_dl = DataLoader(train_ds, batch_size= batch_size, 
                          shuffle=True, collate_fn= collate_fn_r3d_18)
    test_dl = DataLoader(test_ds, batch_size= 2*batch_size, 
                         shuffle=False, collate_fn= collate_fn_r3d_18)

from torch import nn
class ConvRnn(nn.Module):
    def __init__(self, params_model):
        super(ConvRnn, self).__init__()
        num_classes = params_model["num_classes"]
        dr_rate= params_model["dr_rate"]
        pretrained = params_model["pretrained"]
        rnn_hidden_size = params_model["rnn_hidden_size"]
        rnn_num_layers = params_model["rnn_num_layers"]
        
        baseModel = models.densenet121(pretrained=True)
        #num_features = baseModel.fc.in_features #RestNet
        num_features = 1000 #AlexNet
        baseModel.fc = Identity()
        self.baseModel = baseModel
        self.dropout= nn.Dropout(dr_rate)
        self.rnn = nn.LSTM(num_features, rnn_hidden_size, rnn_num_layers)
        self.fc1 = nn.Linear(rnn_hidden_size, num_classes)

    def forward(self, x):
        b_z, ts, c, h, w = x.shape
        ii = 0
        y = self.baseModel((x[:,ii]))
        output, (hn, cn) = self.rnn(y.unsqueeze(1))
        for ii in range(1, ts):
            y = self.baseModel((x[:,ii]))
            out, (hn, cn) = self.rnn(y.unsqueeze(1), (hn, cn))
        out = self.dropout(out[:,-1])
        out = self.fc1(out) 
        return out 
    
class Identity(nn.Module):
    def __init__(self):
        super(Identity, self).__init__()
    def forward(self, x):
        return x

from torchvision import models
from torch import nn

if model_type == "rnn":
    params_model={
        "num_classes": 2,
        "dr_rate": 0.1,
        "pretrained" : True,
        "rnn_num_layers": 1,
        "rnn_hidden_size": 100}
    model = ConvRnn(params_model)

with torch.no_grad():
    if model_type=="rnn":
        x = torch.zeros(1, 4, 3, h, w)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)
device

path2weights = "weights.pt"
torch.save(model.state_dict(), path2weights)

from torch import optim
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau

loss_func = nn.CrossEntropyLoss(reduction="sum")
opt = optim.Adam(model.parameters(), lr=1e-5)
lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5,verbose=1)
os.makedirs("./models", exist_ok=True)

params_train={
    "num_epochs":30,
    "optimizer": opt,
    "loss_func": loss_func,
    "train_dl": train_dl,
    "val_dl": test_dl,
    "lr_scheduler": lr_scheduler,
    "path2weights": "./models/weights_"+model_type+".pt",
    }

acc_list_train, precision_list_train, recall_list_train, f1_list_train, cm_list_train = [],[],[],[],[]

def train(train_loader, net, epoch):

  # Training mode
  net.train()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  for batch in train_loader:
    
    dado, rotulo = batch
  
    # Cast do dado na GPU
    dado = dado.to(device)
    rotulo = rotulo.to(device)
    
    # Forward
    ypred = net(dado)
    loss = loss_func(ypred, rotulo)
    epoch_loss.append(loss.cpu().data)

    _, pred = torch.max(ypred, axis=1)
    pred_list.append(pred.cpu().numpy())
    rotulo_list.append(rotulo.cpu().numpy())
    
    # Backpropagation
    opt.zero_grad()
    loss.backward()
    opt.step()
   
  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  acc = jaccard_score(pred_list, rotulo_list)
  precision = precision_score(pred_list, rotulo_list)
  recall = recall_score(pred_list, rotulo_list)
  f1 = f1_score(pred_list, rotulo_list)
  cm = confusion_matrix(pred_list, rotulo_list)

  acc_list_train.append(acc)
  precision_list_train.append(precision)
  recall_list_train.append(recall)
  f1_list_train.append(f1)
  cm_list_train.append(cm)

  end = time.time()
  print('#################### Train ####################')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Precission: %.2f, recall: %2f, F1: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, precision, recall, f1, end-start))
  
  return epoch_loss.mean()

acc_list_test, precision_list_test, recall_list_test, f1_list_test, cm_list_test = [],[],[],[],[]

def validate(test_loader, net, epoch):

  # Evaluation mode
  net.eval()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  with torch.no_grad(): 
    for batch in test_loader:

      dado, rotulo = batch

      # Cast the data to GPU
      dado = dado.to(device)
      rotulo = rotulo.to(device)

      # Forward
      ypred = net(dado)
      loss = loss_func(ypred, rotulo)
      epoch_loss.append(loss.cpu().data)

      _, pred = torch.max(ypred, axis=1)
      pred_list.append(pred.cpu().numpy())
      rotulo_list.append(rotulo.cpu().numpy())

  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  #pred_list = np.concatenate(pred_list, axis=0)
  #rotulo_list = np.concatenate(rotulo_list, axis=0)

  acc = jaccard_score(pred_list, rotulo_list)
  precision = precision_score(pred_list, rotulo_list)
  recall = recall_score(pred_list, rotulo_list)
  f1 = f1_score(pred_list, rotulo_list)
  cm = confusion_matrix(pred_list, rotulo_list)

  acc_list_test.append(acc)
  precision_list_test.append(precision)
  recall_list_test.append(recall)
  f1_list_test.append(f1)
  cm_list_test.append(cm)

  end = time.time()
  print('********** Validate **********')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Precission: %.2f, recall: %2f, F1: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, precision, recall, f1, end-start))
  
  return epoch_loss.mean()

train_losses, test_losses = [], []
for epoch in range(params_train['num_epochs']):
  # Train
  train_losses.append(train(train_dl, model, epoch))
   
  # Validate
  #test_losses.append(validate(test_dl, model, epoch))

print("train")
print("Accuracy =" + str(acc_list_train))
print("Precision =" + str(precision_list_train))
print("Recall =" + str(recall_list_train))
print("F1 =" + str(f1_list_train))
print("Confusion matrix =" + str(cm_list_train))
print("\n")
print("Train Loss =" + str(train_losses))

