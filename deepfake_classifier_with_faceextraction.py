# -*- coding: utf-8 -*-
"""Deepfake Classifier With FaceExtraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NpsvcZvR7RTnN6FSBjS-25RF0zngmwxT
"""

import torch
import torchvision
from torch import nn, optim
import cv2
from torchvision import models

from torch.autograd import Variable

import glob
import numpy as np

import warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning) 

from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms

from sklearn.metrics import accuracy_score
from sklearn.metrics import jaccard_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.utils.multiclass import type_of_target
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

import time, os

from google.colab import drive
drive.mount('/content/drive')

# Hyperparameters
args = {
    'epoch_num': 30,     # number of epochs.
    'lr': 1e-5,           # Learning_Rate.
    'weight_decay': 1e-3, # Regularization.
    'batch_size': 25,     # batch size.
}

# Checking GPU
if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')

print(args['device'])

train_dataset_path = '/content/drive/MyDrive/Extracted_Faces-split/train'
test_dataset_path = '/content/drive/MyDrive/Extracted_Faces-split/test'
validation_dataset_path = '/content/drive/MyDrive/Extracted_Faces-split/val'

mean = [0.4363, 0.4328, 0.3291]
std = [0.2129,0.2075, 0.2038]

train_transform = transforms.Compose([
    transforms.Resize((224,224)), #Option to resize the images from 256 to 224 pixels
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
])

test_transform = transforms.Compose([
    transforms.Resize((224,224)), #Option to resize the images from 256 to 224 pixels
    transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
])

train_dataset = torchvision.datasets.ImageFolder(root = train_dataset_path, transform = train_transform)
test_dataset = torchvision.datasets.ImageFolder(root = test_dataset_path, transform = test_transform)
validation_dataset = torchvision.datasets.ImageFolder(root = validation_dataset_path, transform = test_transform)

def show_transformed_images(dataset):
    loader = torch.utils.data.DataLoader(dataset, batch_size = args['batch_size'], shuffle = True)
    batch = next(iter(loader))
    images, labels = batch
    
    grid = torchvision.utils.make_grid(images, nrow=5)
    plt.figure(figsize = (11,11))
    plt.imshow(np.transpose(grid, (1,2,0)))
    print('Labels: ', labels)

show_transformed_images(train_dataset)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True)
validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=args['batch_size'], shuffle=True)

net = torchvision.models.densenet121(pretrained=True)

num_ftures = net.fc.in_features
net.fc = nn.Linear(num_ftures, 2)

from collections import OrderedDict

classifier = nn.Sequential(OrderedDict([('fcl', nn.Linear(1024, 25)), #AlexNet = 9216 VGG16 = 25088 DenseNet = 1024
                                         ('relu', nn.ReLU()),
                                         ('drop', nn.Dropout(p=0.5)),
                                         ('fc2', nn.Linear(25, 2)), # outuput equals 2
                                         ('output', nn.LogSoftmax(dim=1))]))

net.classifier = classifier

net = net.to(args['device'])

net

criterion = nn.CrossEntropyLoss().to(args['device'])
optimizer = torch.optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])

acc_list_train, precision_list_train, recall_list_train, f1_list_train, cm_list_train = [],[],[],[],[]

def train(train_loader, net, epoch):

  # Training mode
  net.train()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  for batch in train_loader:
    
    dado, rotulo = batch
  
    # Cast do dado na GPU
    dado = dado.to(args['device'])
    rotulo = rotulo.to(args['device'])
    
    # Forward
    ypred = net(dado)
    loss = criterion(ypred, rotulo)
    epoch_loss.append(loss.cpu().data)

    _, pred = torch.max(ypred, axis=1)
    pred_list.append(pred.cpu().numpy())
    rotulo_list.append(rotulo.cpu().numpy())
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
   
  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()
  
  pred_list = np.concatenate(pred_list, axis=0)
  rotulo_list = np.concatenate(rotulo_list, axis=0)

  acc = jaccard_score(pred_list, rotulo_list)
  precision = precision_score(pred_list, rotulo_list)
  recall = recall_score(pred_list, rotulo_list)
  f1 = f1_score(pred_list, rotulo_list)
  cm = confusion_matrix(pred_list, rotulo_list)

  acc_list_train.append(acc)
  precision_list_train.append(precision)
  recall_list_train.append(recall)
  f1_list_train.append(f1)
  cm_list_train.append(cm)

  end = time.time()
  print('#################### Train ####################')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Precission: %.2f, recall: %2f, F1: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, precision, recall, f1, end-start))
  
  return epoch_loss.mean()

acc_list_test, precision_list_test, recall_list_test, f1_list_test, cm_list_test = [],[],[],[],[]

def validate(test_loader, net, epoch):

  # Evaluation mode
  net.eval()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  with torch.no_grad(): 
    for batch in test_loader:

      dado, rotulo = batch

      # Cast the data to GPU
      dado = dado.to(args['device'])
      rotulo = rotulo.to(args['device'])

      # Forward
      ypred = net(dado)
      loss = criterion(ypred, rotulo)
      epoch_loss.append(loss.cpu().data)

      _, pred = torch.max(ypred, axis=1)
      pred_list.append(pred.cpu().numpy())
      rotulo_list.append(rotulo.cpu().numpy())

      if epoch % 5 == 0:
        torch.save(net.state_dict(), 'checkpoint.pth')

  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  pred_list = np.concatenate(pred_list, axis=0)
  rotulo_list = np.concatenate(rotulo_list, axis=0)

  acc = jaccard_score(pred_list, rotulo_list)
  precision = precision_score(pred_list, rotulo_list)
  recall = recall_score(pred_list, rotulo_list)
  f1 = f1_score(pred_list, rotulo_list)
  cm = confusion_matrix(pred_list, rotulo_list)

  acc_list_test.append(acc)
  precision_list_test.append(precision)
  recall_list_test.append(recall)
  f1_list_test.append(f1)
  cm_list_test.append(cm)

  end = time.time()
  print('********** Validate **********')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Precission: %.2f, recall: %2f, F1: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, precision, recall, f1, end-start))
  
  return epoch_loss.mean()

train_losses, test_losses = [], []
for epoch in range(args['epoch_num']):

  # Train
  train_losses.append(train(train_loader, net, epoch))
   
  # Validate
  test_losses.append(validate(validation_loader, net, epoch))

print("train")
print("Accuracy =" + str(acc_list_train))
print("Precision =" + str(precision_list_train))
print("Recall =" + str(recall_list_train))
print("F1 =" + str(f1_list_train))
print("Confusion matrix =" + str(cm_list_train))
print("\n")
print("Train Loss =" + str(train_losses))

print("Test")
print("Accuracy =" + str(acc_list_test))
print("Precision =" + str(precision_list_test))
print("Recall =" + str(recall_list_test))
print("F1 =" + str(f1_list_test))
print("Confusion matrix =" + str(cm_list_test))
print("\n")
print("Test Loss =" + str(test_losses))

